{
 "cells": [
  {
   "cell_type": "code",
   "id": "72a5867161b718bf",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b19b2bc1be1df99a",
   "metadata": {},
   "source": [
    "import pm4py\n",
    "import scipy\n",
    "import stormpy\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13bcc2fe638199ac",
   "metadata": {},
   "source": [
    "from simulation.markov_models import log_parser\n",
    "from simulation.markov_chain import apply as mc_apply\n",
    "from simulation.markov_chain_vis import view_markov_chain, view_resource_markov_chain, view_non_resource_markov_chain\n",
    "import simulation.util as sim_util"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Event log loading and processing",
   "id": "808973a662193b3c"
  },
  {
   "cell_type": "code",
   "id": "bd8dab6828d51878",
   "metadata": {},
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "from simulation.unfold_events import rename_repeating_events\n",
    "\n",
    "# Simulation parameters\n",
    "np.random.seed(42)\n",
    "\n",
    "event_log = pm4py.read_xes('BPI_Challenge_2013_incidents.xes.gz')\n",
    "event_log = event_log.sort_values(['case:concept:name','time:timestamp'])\n",
    "number_of_traces = event_log['case:concept:name'].nunique()\n",
    "subset_el = event_log[['case:concept:name','concept:name','time:timestamp','org:resource','org:role']]\n",
    "subset_el['org:role'] = subset_el['org:role'].fillna('nan_1').apply(lambda x: x.split('_')[0])\n",
    "subset_el['org:role'] = subset_el['org:role'].replace({'C':'C1','D':'D1','E':'E1'})\n",
    "df = subset_el\n",
    "\n",
    "epsilon = 1\n",
    "final_states = ['Completed']\n",
    "if epsilon>1:\n",
    "    df, final_states = rename_repeating_events(df, epsilon, final_states)\n",
    "subset_el = df\n",
    "# Time unit configuration: choose \"seconds\" or \"hours\"\n",
    "time_unit = \"hours\"  # or \"seconds\"\n",
    "time_factor = 1 if time_unit == \"seconds\" else 1 / 3600  # seconds to chosen unit"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "subset_el['concept:name'].unique()",
   "id": "36082195c4f2eeb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from simulation.timings import Timings\n",
    "\n",
    "timings = Timings()\n",
    "resource_input_array = timings.create_resource_input_array_from_log(subset_el)\n",
    "res_timings = timings.get_timings_per_resource(subset_el, resource_input_array,time_factor=time_factor)\n",
    "times_dictionary = res_timings"
   ],
   "id": "ce93e5358dbe8d36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(times_dictionary.keys()))\n",
    "print(times_dictionary.keys())"
   ],
   "id": "263e23de2d2d8976",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time_diffs = times_dictionary",
   "id": "b0e4f9e9df5eda7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot = False",
   "id": "6613a9326a1719d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exponential fit and statistical hypothesis tests",
   "id": "fd5bd3f93c12efa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import expon, sem, t, entropy, kstest\n",
    "from scipy.special import rel_entr, kl_div\n",
    "from scipy.stats import wasserstein_distance\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def compute_freedman_diaconis_bins(data):\n",
    "    iqr = np.subtract(*np.percentile(data, [75, 25]))\n",
    "    bin_width = 2 * iqr / (n ** (1/3))\n",
    "    return int(np.ceil((np.max(data) - np.min(data)) / bin_width)) if bin_width > 0 else 1\n",
    "\n",
    "# ---- Plot and Compute Distances ----\n",
    "if plot:\n",
    "    fig, axes = plt.subplots(1, len(time_diffs), figsize=(25, 5), tight_layout=True, sharey=True)\n",
    "    if len(time_diffs) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "metrics_all = {}\n",
    "\n",
    "for i, (pair, deltas) in enumerate(time_diffs.items()):\n",
    "    deltas = np.array(deltas)\n",
    "    mean_time = deltas.mean()\n",
    "    print(pair, mean_time)\n",
    "    n = len(deltas)\n",
    "\n",
    "    # Manual fit\n",
    "    rate_manual = 1 / mean_time\n",
    "    ci_half_width = t.ppf(0.975, df=n-1) * sem(deltas)\n",
    "    lower_rate = 1 / (mean_time + ci_half_width)\n",
    "    upper_rate = 1 / (mean_time - ci_half_width)\n",
    "\n",
    "    # Scipy fit\n",
    "    loc, scale = expon.fit(deltas, floc=0)\n",
    "    rate_scipy = 1 / scale\n",
    "\n",
    "    # Histogram\n",
    "    num_bins = compute_freedman_diaconis_bins(deltas)\n",
    "    counts, bin_edges = np.histogram(deltas, bins=num_bins, density=False)\n",
    "    bin_widths = np.diff(bin_edges)\n",
    "    total = np.sum(counts)\n",
    "    hist_probs = counts / total\n",
    "\n",
    "    # Model probabilities over bins\n",
    "    model_probs = expon.cdf(bin_edges[1:], scale=1 / rate_scipy) - expon.cdf(bin_edges[:-1], scale=1 / rate_scipy)\n",
    "    eps = 1e-12\n",
    "    # kldiv = np.sum(rel_entr(hist_probs, model_probs))\n",
    "    test_kl_div = entropy(hist_probs, model_probs)\n",
    "    test2_kl_div = np.sum(kl_div(hist_probs, model_probs))\n",
    "    ks_test, ks_p_value = kstest(deltas, lambda deltas: expon.cdf(deltas,loc,scale))\n",
    "    m = 0.5 * (hist_probs + model_probs)\n",
    "    js_div = 0.5 * (entropy(hist_probs + eps, m + eps) + entropy(model_probs + eps, m + eps))\n",
    "    tv_dist = 0.5 * np.sum(np.abs(hist_probs - model_probs))\n",
    "    w_dist = wasserstein_distance(deltas, expon.rvs(scale=1 / rate_scipy, size=len(deltas), random_state=42))\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_all[pair] = {\n",
    "        \"kl_divergence\": test_kl_div,\n",
    "        \"js_divergence\": js_div,\n",
    "        \"total_variation\": tv_dist,\n",
    "        \"wasserstein_distance\": w_dist,\n",
    "        \"ks_test\": ks_test,\n",
    "        \"ks_p_value\": ks_p_value,\n",
    "        \"n\": n\n",
    "    }\n",
    "\n",
    "    # Plot histogram and fits\n",
    "    if plot:\n",
    "        ax = axes[i]\n",
    "        sns.histplot(deltas, bins=num_bins, stat=\"density\", ax=ax, color=\"skyblue\", label=\"Empirical\")\n",
    "        x_vals = np.linspace(0, max(deltas) * 1.2, 200)\n",
    "        ax.plot(x_vals, expon.pdf(x_vals, scale=1 / rate_scipy), linestyle=\"-.\", color=\"purple\",\n",
    "                label=f\"Fitted λ = {rate_scipy:.4f}\")\n",
    "        ax.fill_between(x_vals,\n",
    "                        expon.pdf(x_vals, scale=1 / lower_rate),\n",
    "                        expon.pdf(x_vals, scale=1 / upper_rate),\n",
    "                        color=\"gray\", alpha=0.3, label=\"95% CI band\")\n",
    "\n",
    "        ax.set_title(f\"{pair[0]} → {pair[2]} → {pair[1]}\")\n",
    "        ax.set_xlabel(f\"Time ({time_unit})\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.legend()\n",
    "\n",
    "        # Text box with metrics\n",
    "        textstr = 'Metrics:\\n'\n",
    "        textstr += '\\n'.join([\n",
    "            f\"KL: {test_kl_div:.4f}\",\n",
    "            f\"JS: {js_div:.4f}\",\n",
    "            f\"TV: {tv_dist:.4f}\",\n",
    "            f\"W: {w_dist:.4f}\",\n",
    "            f\"KS: {ks_test:.4f} P: {ks_p_value:.4f}\",\n",
    "            f\"N: {n}\"\n",
    "        ])\n",
    "        ax.text(0.98, 0.55, textstr,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=9,\n",
    "                verticalalignment='top',\n",
    "                horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "if plot:\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# If needed later:\n",
    "# print(metrics_all)"
   ],
   "id": "19fbde6063d91142",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import combine_pvalues\n",
    "\n",
    "total_n = sum(m[\"n\"] for m in metrics_all.values())\n",
    "\n",
    "# Weighted averages for divergence metrics\n",
    "kl_weighted = sum(m[\"kl_divergence\"] * m[\"n\"] for m in metrics_all.values()) / total_n\n",
    "js_weighted = sum(m[\"js_divergence\"] * m[\"n\"] for m in metrics_all.values()) / total_n\n",
    "\n",
    "# Simple averages for bounded or scale-sensitive distances\n",
    "tv_average = np.mean([m[\"total_variation\"] for m in metrics_all.values()])\n",
    "w_average = np.mean([m[\"wasserstein_distance\"] for m in metrics_all.values()])\n",
    "\n",
    "ks_average = np.mean([m[\"ks_test\"] for m in metrics_all.values()])\n",
    "stat, ks_combined_p = combine_pvalues([m[\"ks_p_value\"]+eps for m in metrics_all.values()])\n",
    "# Final aggregate summary\n",
    "aggregate_metrics = {\n",
    "    \"KL Divergence (weighted)\": kl_weighted,\n",
    "    \"JS Divergence (weighted)\": js_weighted,\n",
    "    \"Total Variation Distance (mean)\": tv_average,\n",
    "    \"Wasserstein Distance (mean)\": w_average,\n",
    "    \"KS Test (mean)\": ks_average,\n",
    "    \"KS p-value (combined fisher)\": ks_combined_p,\n",
    "    \"Total Samples\": total_n\n",
    "}\n",
    "\n",
    "# Print aggregated metrics\n",
    "print(\"\\n--- Aggregated Metrics ---\")\n",
    "for key, val in aggregate_metrics.items():\n",
    "    print(f\"{key}: {val:.4f}\")\n",
    "\n",
    "# ---- Display in a New Figure ----\n",
    "fig_summary, ax_summary = plt.subplots(figsize=(5, 4))\n",
    "ax_summary.axis(\"off\")\n",
    "\n",
    "summary_text = '\\n'.join([\n",
    "    f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\"\n",
    "    for k, v in aggregate_metrics.items()\n",
    "])\n",
    "\n",
    "ax_summary.text(0.5, 0.5, summary_text,\n",
    "                fontsize=11,\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.9))\n",
    "\n",
    "ax_summary.set_title(\"Aggregated Fit Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1176debdbc0f67bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building the ctmc and running it",
   "id": "f6978a863c754ab1"
  },
  {
   "cell_type": "code",
   "id": "1000ef66fe0667e6",
   "metadata": {},
   "source": [
    "from simulation.unfold_events import rename_repeating_events\n",
    "\n",
    "df, final_states = rename_repeating_events(df,epsilon,final_states)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96be41a16a217a8",
   "metadata": {},
   "source": [
    "subset_el = pm4py.convert_to_event_log(df)\n",
    "subset_el = log_parser.add_start_end(subset_el)\n",
    "dfg, start_activities, end_activities = pm4py.discover_dfg(subset_el)\n",
    "dfg[\"end\", \"start\"] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1a301b90d3dd19e",
   "metadata": {},
   "source": [
    "pm4py.view_dfg(dfg, start_activities, end_activities)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b13a70b666c2a8a",
   "metadata": {},
   "source": [
    "subset_el = pm4py.convert_to_dataframe(subset_el)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "418f1fe7074ccb93",
   "metadata": {},
   "source": [
    "data_transition_role_frequency = sim_util.get_transition_resource_dict(subset_el)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea7e0ced8a76c922",
   "metadata": {},
   "source": [
    "data_mean_transition_role_time = {}\n",
    "tuples_to_discard = set()\n",
    "for k,v in data_transition_role_frequency.items():\n",
    "    if k in ['start','end']:\n",
    "        continue\n",
    "    for k2,v2 in v.items():\n",
    "        if k2 in ['start','end']:\n",
    "            continue\n",
    "        all_freq = 0\n",
    "        for k3,v3 in v2.items():\n",
    "            all_freq += v3\n",
    "            if (k,k2,k3) in times_dictionary:\n",
    "                times = times_dictionary[(k,k2,k3)]\n",
    "                times = np.array(times)\n",
    "                times = times/3600\n",
    "                times = times[times != 0]\n",
    "                if len(times) > 1: # only take times that have more than 1 value\n",
    "                    expon_loc, expon_scale = scipy.stats.expon.fit(times)\n",
    "\n",
    "                    # f = Fitter(times, distributions=['expon'])\n",
    "                    # f.fit()\n",
    "                    # best = f.get_best()['expon']\n",
    "                    # expon_loc_fitter, expon_scale_fitter = best['loc'], best['scale']\n",
    "\n",
    "                    if expon_scale>0: # do not take times that cannot be fit into an exponential\n",
    "                        rate = 1/expon_scale\n",
    "                        if k not in data_mean_transition_role_time:\n",
    "                            data_mean_transition_role_time[k] = {}\n",
    "                        if k2 not in data_mean_transition_role_time[k]:\n",
    "                            data_mean_transition_role_time[k][k2] = {}\n",
    "                        if k3 not in data_mean_transition_role_time[k][k2]:\n",
    "                            data_mean_transition_role_time[k][k2][k3] = {\n",
    "                                # 'loc': expon_loc_fitter,\n",
    "                                # 'scale': expon_scale_fitter,\n",
    "                                'loc': expon_loc,\n",
    "                                'scale': expon_scale,\n",
    "                                'lambda': rate\n",
    "                            }\n",
    "                    else:\n",
    "                        print(f\"[No exponential!] {k},{k2},{k3}\")\n",
    "                        tuples_to_discard.add((k,k2,k3))\n",
    "                        print(times)\n",
    "                else:\n",
    "                    print(f\"[No times!] {k},{k2},{k3}\")\n",
    "                    tuples_to_discard.add((k,k2,k3))\n",
    "                    print(times)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e84c6508fe0da157",
   "metadata": {},
   "source": [
    "for (e_from,e_to,role) in tuples_to_discard:\n",
    "    if e_from in data_transition_role_frequency:\n",
    "        if e_to in data_transition_role_frequency[e_from]:\n",
    "            if role in data_transition_role_frequency[e_from][e_to]:\n",
    "                data_transition_role_frequency[e_from][e_to].pop(role)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce44771e0e586256",
   "metadata": {},
   "source": [
    "for e_from in data_transition_role_frequency.keys():\n",
    "    for e_to in data_transition_role_frequency.keys():\n",
    "        if (e_from == 'start' and e_to == 'start') or (e_from == 'end' and e_to == 'end'):\n",
    "            data_transition_role_frequency[e_from].pop(e_to)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f660e252723ea3d",
   "metadata": {},
   "source": [
    "def remove_empty_keys(d):\n",
    "    \"\"\"Recursively remove empty keys from a three-level nested dictionary.\"\"\"\n",
    "    if not isinstance(d, dict):\n",
    "        return d  # Return non-dict values as they are\n",
    "\n",
    "    cleaned_dict = {}\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            cleaned_value = remove_empty_keys(value)  # Recursively clean sub-dictionaries\n",
    "            if cleaned_value:  # Add only if not empty\n",
    "                cleaned_dict[key] = cleaned_value\n",
    "        elif value not in (None, \"\", [], {}, ()):  # Ignore empty values\n",
    "            cleaned_dict[key] = value\n",
    "\n",
    "    return cleaned_dict\n",
    "\n",
    "data_transition_role_frequency = remove_empty_keys(data_transition_role_frequency)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9521bb4d2101842",
   "metadata": {},
   "source": [
    "role_resources = sim_util.get_detailed_weighted_role(subset_el)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f89c08e0ea52aee0",
   "metadata": {},
   "source": [
    "role_trials = {k:int(v) for k,v in role_resources.items()}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "279afab90356e582",
   "metadata": {},
   "source": [
    "res = {}\n",
    "out_frequency = {}\n",
    "data_transition_role_prob = {}\n",
    "\n",
    "for k,v in data_transition_role_frequency.items():\n",
    "    if k in ['start','end']:\n",
    "        continue\n",
    "    out_freq = 0\n",
    "    if k not in data_transition_role_prob:\n",
    "        data_transition_role_prob[k] = {}\n",
    "\n",
    "    for k2,v2 in v.items():\n",
    "        if k2 in ['start','end']:\n",
    "            continue\n",
    "        all_freq = 0\n",
    "\n",
    "        if k2 not in data_transition_role_prob[k]:\n",
    "            data_transition_role_prob[k][k2] = {}\n",
    "\n",
    "        if k not in res:\n",
    "            res[k] = {}\n",
    "        if k2 not in res[k]:\n",
    "            for k3,v3 in v2.items():\n",
    "                if k3 not in data_transition_role_prob[k][k2]:\n",
    "                    data_transition_role_prob[k][k2][k3] = v3\n",
    "                all_freq += v3\n",
    "            res[k][k2] = all_freq\n",
    "            out_freq += all_freq\n",
    "        out_frequency[k] = out_freq\n",
    "\n",
    "for k,v in res.items():\n",
    "    for k2,v2 in v.items():\n",
    "        res[k][k2] = res[k][k2]/out_frequency[k]\n",
    "\n",
    "for k,v in data_transition_role_prob.items():\n",
    "    for k2,v2 in v.items():\n",
    "        for k3,v3 in v2.items():\n",
    "            data_transition_role_prob[k][k2][k3] = v3/out_frequency[k]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0af8ca520b5dc71",
   "metadata": {},
   "source": [
    "view_resource_markov_chain(data_transition_role_prob)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1afadfeeadf40ad",
   "metadata": {},
   "source": "view_non_resource_markov_chain(res)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd6b38808cd08069",
   "metadata": {},
   "source": [
    "states = set(subset_el['concept:name'].unique()).difference(set(['start','end']))\n",
    "n = len(states)\n",
    "i = 0\n",
    "correspondence = {s:i for s,i in zip(states,range(len(states)))}\n",
    "#TODO: make sure none of the final states have state = 0 in the prism program\n",
    "non_final_states = list(states.difference(set(final_states)))\n",
    "for s in final_states:\n",
    "    if correspondence[s] == 0:\n",
    "        correspondence[s] = correspondence[non_final_states[0]]\n",
    "        correspondence[non_final_states[0]] = 0\n",
    "correspondence"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6fce727b45e682d",
   "metadata": {},
   "source": [
    "from simulation.ctmc import create_prism_program_from_log\n",
    "\n",
    "probabilities = create_prism_program_from_log(\n",
    "                            correspondence,\n",
    "                            final_states,\n",
    "                            data_mean_transition_role_time,\n",
    "                            role_resources,\n",
    "                            data_transition_role_frequency,\n",
    "                            role_trials,\n",
    "                            'ctmc-bpic13.sm')\n",
    "# print(probabilities)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46bc4cbd5574daa0",
   "metadata": {},
   "source": "prism_program = stormpy.parse_prism_program('ctmc-bpic13.sm',prism_compat=True,simplify=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = stormpy.build_model(prism_program)\n",
    "# print(\"Number of states: {}\".format(model.nr_states))\n",
    "# print(\"Number of transitions: {}\".format(model.nr_transitions))\n",
    "# print(\"Labels: {}\".format(model.labeling.get_labels()))\n",
    "\n",
    "\n",
    "# formula_str = f'R=? [F {labels}]'\n",
    "# formula_str = f'Rmin=? [C]'\n",
    "\n",
    "def get_result(model, prism_program):\n",
    "    labels = \"\"\n",
    "    for fs in final_states:\n",
    "        labels += f'\"q_terminal_{fs}\" |'\n",
    "    labels = labels[:-2]\n",
    "    formula_str = f'Tmin=? [F {labels}]'\n",
    "    print(final_states)\n",
    "    properties = stormpy.parse_properties(formula_str, prism_program)\n",
    "    result = stormpy.model_checking(model, properties[0])\n",
    "    initial_state = model.initial_states[0]\n",
    "    result = result.at(initial_state)\n",
    "    print(f\"Hours: {result}\")\n",
    "    if result<np.inf:\n",
    "        if time_unit == 'hours':\n",
    "            print(f\"Duration: {timedelta(hours=result)}\")\n",
    "        else:\n",
    "            print(f\"Duration: {timedelta(seconds=result)}\")\n",
    "    return result\n",
    "\n",
    "result = get_result(model,prism_program)"
   ],
   "id": "52337ff3f67c4a78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis between ground truth, result and metrics",
   "id": "39c98301831e10dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_all",
   "id": "1adcba6d55452fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "durations = []\n",
    "# x = list(range(1,50))\n",
    "samples = 500\n",
    "for i in range(samples):\n",
    "    regression_role_trials = {}\n",
    "    for k,v in role_trials.items():\n",
    "        random_resource_number = abs(random.gauss(v,v/2))\n",
    "        regression_role_trials[k] = random_resource_number\n",
    "    probabilities = create_prism_program_from_log(\n",
    "                            correspondence,\n",
    "                            final_states,\n",
    "                            data_mean_transition_role_time,\n",
    "                            role_resources,\n",
    "                            data_transition_role_frequency,\n",
    "                            regression_role_trials,\n",
    "                            'ctmc.sm')\n",
    "    prism_program = stormpy.parse_prism_program('ctmc.sm', prism_compat=True, simplify=True)\n",
    "    model = stormpy.build_model(prism_program)\n",
    "    labels = \"\"\n",
    "    for fs in final_states:\n",
    "        labels += f'\"q_terminal_{fs}\" |'\n",
    "    labels = labels[:-2]\n",
    "\n",
    "    formula_str = f'Tmin=? [F {labels}]'\n",
    "    properties = stormpy.parse_properties(formula_str, prism_program)\n",
    "    result = stormpy.model_checking(model, properties[0])\n",
    "    initial_state = model.initial_states[0]\n",
    "    result = result.at(initial_state)\n",
    "    durations.append({**regression_role_trials, \"duration\": result})\n",
    "    # print(f'{i}/{samples}')"
   ],
   "id": "153b9fa0424a927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "durations_df = pd.DataFrame(durations)\n",
    "durations_df"
   ],
   "id": "85f6a30ca6be1715",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example: Load your DataFrame\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "def run_linear_regression(df, target_column, fit_intercept=True):\n",
    "    # Split into X (features) and y (target)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Fit model\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Print model coefficients\n",
    "    intercept = model.intercept_\n",
    "    coef = model.coef_\n",
    "\n",
    "    print(\"Intercept:\", intercept if fit_intercept else \"Not used\")\n",
    "    print(\"Coefficients:\")\n",
    "    for col, weight in zip(X.columns, coef):\n",
    "        print(f\"{col}: {weight:.4f}\")\n",
    "\n",
    "    return model, X.columns, coef\n",
    "\n",
    "def rank_features_by_importance(X, coef):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X,\n",
    "        'Coefficient': coef,\n",
    "        'Importance (abs)': abs(coef)\n",
    "    })\n",
    "    return importance_df.sort_values(by='Importance (abs)', ascending=False)\n",
    "\n",
    "# Run regression\n",
    "model, features, coefs = run_linear_regression(durations_df, 'duration', fit_intercept=False)\n",
    "\n",
    "# Rank features\n",
    "ranking = rank_features_by_importance(features, coefs)\n",
    "print(\"\\nFeature Ranking:\\n\", ranking)"
   ],
   "id": "2c4d60f2626ffc3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Other stuff and old stuff not used",
   "id": "d807edac1045ebf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract time deltas in desired unit\n",
    "time_diffs = {}\n",
    "for case_id, group in df.groupby(\"case:concept:name\"):\n",
    "    events = group[\"concept:name\"].tolist()\n",
    "    times = group[\"time:timestamp\"].tolist()\n",
    "    for i in range(len(events) - 1):\n",
    "        pair = (events[i], events[i + 1])\n",
    "        delta_time = (times[i + 1] - times[i]).total_seconds() * time_factor\n",
    "        time_diffs.setdefault(pair, []).append(delta_time)"
   ],
   "id": "f1c45bf629f44b28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot histogram and exponential fits\n",
    "fig, axes = plt.subplots(1, len(time_diffs), figsize=(14, 5))\n",
    "\n",
    "for ax, (pair, deltas) in zip(axes, time_diffs.items()):\n",
    "    deltas = np.array(deltas)\n",
    "    mean_time = deltas.mean()\n",
    "    n = len(deltas)\n",
    "\n",
    "    # Manual exponential fit\n",
    "    rate_manual = 1 / mean_time\n",
    "    ci_half_width = t.ppf(0.975, df=n-1) * sem(deltas)\n",
    "    lower_rate = 1 / (mean_time + ci_half_width)\n",
    "    upper_rate = 1 / (mean_time - ci_half_width)\n",
    "\n",
    "    # Scipy fit with loc fixed to 0\n",
    "    loc, scale = expon.fit(deltas, floc=0)\n",
    "    rate_scipy = 1 / scale\n",
    "\n",
    "    # Plot histogram\n",
    "    sns.histplot(deltas, bins=10, stat=\"density\", ax=ax, color=\"skyblue\", label=\"Empirical\")\n",
    "\n",
    "    # X values for plotting\n",
    "    x_vals = np.linspace(0, max(deltas) * 1.2, 200)\n",
    "\n",
    "    # PDF curves\n",
    "    # ax.plot(x_vals, expon.pdf(x_vals, scale=1 / rate_manual), color=\"darkblue\",\n",
    "    #         label=f\"Fitted λ (manual) = {rate_manual:.4f}\")\n",
    "    ax.plot(x_vals, expon.pdf(x_vals, scale=1 / rate_scipy), linestyle=\"-.\", color=\"purple\",\n",
    "        label=f\"Fitted λ (scipy) = {rate_scipy:.4f}\")\n",
    "    ax.fill_between(x_vals,\n",
    "                    expon.pdf(x_vals, scale=1 / lower_rate),\n",
    "                    expon.pdf(x_vals, scale=1 / upper_rate),\n",
    "                    color=\"gray\", alpha=0.3, label=\"95% CI band\")\n",
    "\n",
    "    # Labels and titles\n",
    "    ax.set_title(f\"{pair[0]} → {pair[1]}\")\n",
    "    ax.set_xlabel(f\"Time ({time_unit})\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5f2fd197790f3d97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_times_event_log(log_input):\n",
    "    if type(log_input) == pd.DataFrame:\n",
    "        log = pm4py.convert_to_event_log(deepcopy(log_input))\n",
    "    else:\n",
    "        log = deepcopy(log_input)\n",
    "    times = []\n",
    "    for trace in log:\n",
    "        start = trace[0]['time:timestamp']\n",
    "        end = trace[len(trace)-1] ['time:timestamp']\n",
    "        time = end - start\n",
    "        times.append(time.total_seconds())\n",
    "    return times\n",
    "\n",
    "def extract_times_with_future(log_input):\n",
    "    times_dictionary = {}\n",
    "    if type(log_input) == pd.DataFrame:\n",
    "        log = pm4py.convert_to_event_log(deepcopy(log_input))\n",
    "    else:\n",
    "        log = deepcopy(log_input)\n",
    "    for trace in log:\n",
    "        first = True\n",
    "        for next_event in trace:\n",
    "            if not first and next_event['concept:name'] != 'end' and event['concept:name'] != 'start':\n",
    "                time = next_event['time:timestamp'] - event['time:timestamp']\n",
    "                if not (event['concept:name'], next_event['concept:name']) in times_dictionary.keys():\n",
    "                    times_dictionary[(event['concept:name'], next_event['concept:name'])] = [time.total_seconds()]\n",
    "                else:\n",
    "                    times_dictionary[(event['concept:name'], next_event['concept:name'])].append(time.total_seconds())\n",
    "            event = next_event\n",
    "            first = False\n",
    "    return times_dictionary"
   ],
   "id": "c25b19a934c7d67a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False:\n",
    "    semi_markov_json = mc_apply(subset_el)\n",
    "    view_markov_chain(semi_markov_json)"
   ],
   "id": "8fcc840cad61cac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "670f27aee18d937f",
   "metadata": {},
   "source": [
    "# prism_program = stormpy.parse_prism_program('ctmc_simple_choice.sm',prism_compat=True,simplify=True)\n",
    "prism_program = stormpy.parse_prism_program('ctmc_basic.sm',prism_compat=True,simplify=True)\n",
    "model = stormpy.build_model(prism_program)\n",
    "# print(\"Number of states: {}\".format(model.nr_states))\n",
    "# print(\"Number of transitions: {}\".format(model.nr_transitions))\n",
    "# print(\"Labels: {}\".format(model.labeling.get_labels()))\n",
    "labels = \"\"\n",
    "for fs in final_states:\n",
    "    labels += f'\"q_terminal_{fs}\" |'\n",
    "labels = labels[:-2]\n",
    "\n",
    "formula_str = f'R=? [F {labels}]'\n",
    "properties = stormpy.parse_properties(formula_str, prism_program)\n",
    "result = stormpy.model_checking(model, properties[0])\n",
    "initial_state = model.initial_states[0]\n",
    "result = result.at(initial_state)\n",
    "print(f\"Hours: {result}\")\n",
    "if result<np.inf:\n",
    "    print(f\"Duration: {timedelta(seconds=result)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a60daf69064d45",
   "metadata": {},
   "source": [
    "data_mean_transition_role_time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f28af840d6ed6b18",
   "metadata": {},
   "source": [
    "pm4py.get_cycle_time(subset_el)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96c90e8a4eeb981",
   "metadata": {},
   "source": [
    "pm4py.get_all_case_durations(subset_el)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ef1756524c4dc5a",
   "metadata": {},
   "source": [
    "mean, median, margin_of_error = sim_util.get_pm4py_reference_times(subset_el)\n",
    "print(timedelta(seconds=median))\n",
    "print(timedelta(seconds=mean))\n",
    "print(timedelta(seconds=margin_of_error))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d52e97f4a35afbe2",
   "metadata": {},
   "source": [
    "middle_mean = np.mean(times_dictionary[('Start','Middle','R1')]) + np.mean(times_dictionary[('Middle','End','R1')])\n",
    "start_end_mean = np.mean(times_dictionary[('Start','End','R1')])\n",
    "mean_all = np.mean([middle_mean, start_end_mean])\n",
    "print(timedelta(seconds=mean_all))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81aeaf1858681fa0",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d63b9c1-aa8d-42e0-934d-fc3247b50fcd",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bbc7c990-93ee-4eaa-8f31-38a6567619cf",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "973c61e3-138c-4cef-9b5f-c74fda393838",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36589180-39a1-4a3d-93dd-43ef2c7955cd",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b1d8608-0acb-432a-9504-3fa28d5e52cb",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12e936fb-113f-4936-ac44-c61a0ce71c1f",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6a678a1-a88f-439e-b211-4d1edb3e4ac9",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
