{
 "cells": [
  {
   "cell_type": "code",
   "id": "72a5867161b718bf",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b19b2bc1be1df99a",
   "metadata": {},
   "source": [
    "import pm4py\n",
    "import scipy\n",
    "import stormpy\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13bcc2fe638199ac",
   "metadata": {},
   "source": [
    "from simulation.markov_models import log_parser\n",
    "from simulation.markov_chain import apply as mc_apply\n",
    "from simulation.markov_chain_vis import view_markov_chain, view_resource_markov_chain, view_non_resource_markov_chain\n",
    "import simulation.util as sim_util"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BPIC13 event log",
   "id": "808973a662193b3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "event_log = pm4py.read_xes('BPI_Challenge_2013_incidents.xes.gz')\n",
    "event_log = event_log.sort_values(['case:concept:name','time:timestamp'])\n",
    "number_of_traces = event_log['case:concept:name'].nunique()\n",
    "subset_el = event_log[['case:concept:name','concept:name','time:timestamp','org:resource','org:role']]\n",
    "subset_el['org:role'] = subset_el['org:role'].fillna('nan_1')\n",
    "subset_el['org:role'] = subset_el['org:role'].apply(lambda x: x.split('_')[0])\n",
    "subset_el['org:role'] = subset_el['org:role'].replace({'C':'C1','D':'D1','E':'E1'})\n",
    "# subset_el['org:role'] = subset_el['org:role'].replace({'C':'nan','D':'nan','E':'nan','V3':'nan','A2':'nan'})\n",
    "# subset_el['org:resource'] = 'Bob'\n",
    "\n",
    "df = subset_el\n",
    "\n",
    "df[\"time:timestamp\"] = pd.to_datetime(df[\"time:timestamp\"])\n",
    "df = df.sort_values(by=[\"case:concept:name\", \"time:timestamp\"]).reset_index(drop=True)\n",
    "epsilon = 1\n",
    "# time_unit = 'hours'\n",
    "# time_factor = 1/3600 # hours\n",
    "final_states = ['Completed']\n",
    "\n",
    "from simulation.unfold_events import rename_repeating_events\n",
    "if epsilon > 1:\n",
    "    df, final_states = rename_repeating_events(df,epsilon,final_states)\n",
    "    print(final_states)"
   ],
   "id": "511f48658bda4621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_diffs = {}\n",
    "for case_id, group in df.groupby(\"case:concept:name\"):\n",
    "    events = group[\"concept:name\"].tolist()\n",
    "    roles = group[\"org:role\"].tolist()\n",
    "    times = group[\"time:timestamp\"].tolist()\n",
    "    for i in range(len(events) - 1):\n",
    "        pair = (events[i], events[i + 1], roles[i + 1])\n",
    "        delta_time = (times[i + 1] - times[i]).total_seconds()/3600\n",
    "        time_diffs.setdefault(pair, []).append(delta_time)\n",
    "times_dictionary = time_diffs"
   ],
   "id": "c5b63f10e7a4cfc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exponential fit and statistical hypothesis tests",
   "id": "fd5bd3f93c12efa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import expon, sem, t, entropy, kstest\n",
    "from scipy.special import rel_entr, kl_div\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from fitter import Fitter, get_common_distributions\n",
    "\n",
    "def compute_freedman_diaconis_bins(data):\n",
    "    iqr = np.subtract(*np.percentile(data, [75, 25]))\n",
    "    bin_width = 2 * iqr / (n ** (1/3))\n",
    "    return int(np.ceil((np.max(data) - np.min(data)) / bin_width)) if bin_width > 0 else 1\n",
    "\n",
    "metrics_all = {}\n",
    "\n",
    "data_mean_transition_role_time = {}\n",
    "tuples_to_discard = set()\n",
    "\n",
    "plot = False\n",
    "\n",
    "sum_of_ssrs = 0\n",
    "abs_sum_of_ssrs = 0\n",
    "\n",
    "for i, (pair, deltas) in enumerate(times_dictionary.items()):\n",
    "    deltas = np.array(deltas)\n",
    "\n",
    "    mean_time = deltas.mean() # equivalent to 1/expon.fit(deltas, floc=0)\n",
    "    n = len(set(deltas))\n",
    "    if n > 3:\n",
    "        loc, scale = expon.fit(deltas) # equivalent to the Fitter fit\n",
    "        rate = 1/scale\n",
    "        k,k2,k3 = pair\n",
    "        if k not in data_mean_transition_role_time:\n",
    "            data_mean_transition_role_time[k] = {}\n",
    "        if k2 not in data_mean_transition_role_time[k]:\n",
    "            data_mean_transition_role_time[k][k2] = {}\n",
    "        if k3 not in data_mean_transition_role_time[k][k2]:\n",
    "            data_mean_transition_role_time[k][k2][k3] = {\n",
    "                'lambda': rate\n",
    "            }\n",
    "\n",
    "        # Histogram\n",
    "        num_bins = compute_freedman_diaconis_bins(deltas)\n",
    "        counts, bin_edges = np.histogram(deltas, bins=num_bins, density=True)\n",
    "        bin_widths = np.diff(bin_edges)\n",
    "        total = np.sum(counts)\n",
    "        hist_probs = counts / total\n",
    "\n",
    "        # Model probabilities over bins\n",
    "        model_probs = expon.cdf(bin_edges[1:], scale=scale) - expon.cdf(bin_edges[:-1], scale=scale)\n",
    "        eps = 1e-12\n",
    "        # kldiv = np.sum(rel_entr(hist_probs, model_probs))\n",
    "        test_kl_div = entropy(hist_probs + eps, model_probs + eps)\n",
    "        test2_kl_div = np.sum(kl_div(hist_probs + eps, model_probs + eps))\n",
    "        ks_test, ks_p_value = kstest(deltas + eps, lambda deltas: expon.cdf(deltas,loc,scale)+eps)\n",
    "        m = 0.5 * (hist_probs + model_probs)\n",
    "        js_div = 0.5 * (entropy(hist_probs + eps, m + eps) + entropy(model_probs + eps, m + eps))\n",
    "        tv_dist = 0.5 * np.sum(np.abs(hist_probs - model_probs))\n",
    "        w_dist = wasserstein_distance(deltas, expon.rvs(scale=scale, size=len(deltas), random_state=42))\n",
    "\n",
    "        # Save metrics\n",
    "        metrics_all[pair] = {\n",
    "            \"rate\": scale,\n",
    "            \"kl_divergence\": test_kl_div,\n",
    "            \"js_divergence\": js_div,\n",
    "            \"total_variation\": tv_dist,\n",
    "            \"wasserstein_distance\": w_dist,\n",
    "            \"ks_test\": ks_test,\n",
    "            \"ks_p_value\": ks_p_value,\n",
    "            \"n\": n\n",
    "        }\n",
    "\n",
    "        # Plot histogram and fits\n",
    "        if plot:\n",
    "\n",
    "            # confidence interval\n",
    "            ci_half_width = t.ppf(0.975, df=n-1) * sem(deltas)\n",
    "            lower_rate = 1 / (mean_time + ci_half_width)\n",
    "            upper_rate = 1 / (mean_time - ci_half_width)\n",
    "\n",
    "            fig, ax = plt.subplots(1,1,figsize=(5, 5), tight_layout=True)\n",
    "            sns.histplot(deltas, bins=num_bins, stat=\"density\", ax=ax, color=\"skyblue\", label=\"Empirical\")\n",
    "            x_vals = np.linspace(0, max(deltas) * 1.2, 200)\n",
    "            ax.plot(x_vals, expon.pdf(x_vals, scale=scale), linestyle=\"-.\", color=\"purple\",\n",
    "                    label=f\"Fitted λ = {rate:.4f}\")\n",
    "            ax.fill_between(x_vals,\n",
    "                            expon.pdf(x_vals, scale=1 / lower_rate),\n",
    "                            expon.pdf(x_vals, scale=1 / upper_rate),\n",
    "                            color=\"gray\", alpha=0.3, label=\"95% CI band\")\n",
    "\n",
    "            ax.set_title(f\"{pair[0]} - {pair[2]} → {pair[1]}\")\n",
    "            ax.set_xlabel(f\"Time (hours)\")\n",
    "            ax.set_ylabel(\"Density\")\n",
    "            q1, q3 = np.percentile(deltas, [25, 75])\n",
    "            ax.set_xlim(-0.2,max(deltas))\n",
    "            ax.set_ylim(-0.2,5)\n",
    "            ax.legend()\n",
    "\n",
    "            # Text box with metrics\n",
    "            textstr = 'Metrics:\\n'\n",
    "            textstr += '\\n'.join([\n",
    "                f\"KL: {test_kl_div:.4f}\",\n",
    "                f\"JS: {js_div:.4f}\",\n",
    "                f\"TV: {tv_dist:.4f}\",\n",
    "                f\"W: {w_dist:.4f}\",\n",
    "                f\"KS: {ks_test:.4f} P: {ks_p_value:.4f}\",\n",
    "                f\"N: {n}\"\n",
    "            ])\n",
    "            ax.text(0.98, 0.55, textstr,\n",
    "                    transform=ax.transAxes,\n",
    "                    fontsize=9,\n",
    "                    verticalalignment='top',\n",
    "                    horizontalalignment='right',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            filename = f\"{pair[0]}-{pair[2]}-{pair[1]}.png\"\n",
    "            filepath = os.path.join(\"plots\", filename)\n",
    "            plt.savefig(filepath, format=\"png\")\n",
    "            plt.close()  # Optional but good practice to free memory\n",
    "\n",
    "    else:\n",
    "        print(pair)\n",
    "        #TODO: everything we discard we have to either remove from the DFG frequency or filter the entire event log.\n",
    "        tuples_to_discard.add(pair)"
   ],
   "id": "19fbde6063d91142",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total = len(times_dictionary)\n",
    "kept = len(times_dictionary) - len(tuples_to_discard)\n",
    "discarded = len(tuples_to_discard)\n",
    "\n",
    "print('Kept:', kept)\n",
    "print('Discarded:', discarded)\n",
    "kept_percent = (kept / total) * 100\n",
    "print(f'Kept %: {kept_percent:.2f}%')"
   ],
   "id": "3ae5aca465bfda7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import combine_pvalues\n",
    "\n",
    "total_n = sum(m[\"n\"] for m in metrics_all.values())\n",
    "\n",
    "# Weighted averages for divergence metrics\n",
    "kl_weighted = sum(m[\"kl_divergence\"] * m[\"n\"] for m in metrics_all.values()) / total_n\n",
    "js_weighted = sum(m[\"js_divergence\"] * m[\"n\"] for m in metrics_all.values()) / total_n\n",
    "\n",
    "# Simple averages for bounded or scale-sensitive distances\n",
    "tv_average = np.mean([m[\"total_variation\"] for m in metrics_all.values()])\n",
    "w_average = np.mean([m[\"wasserstein_distance\"] for m in metrics_all.values()])\n",
    "\n",
    "ks_average = np.mean([m[\"ks_test\"] for m in metrics_all.values()])\n",
    "stat, ks_combined_p = combine_pvalues([m[\"ks_p_value\"] for m in metrics_all.values()], method='pearson')\n",
    "# Final aggregate summary\n",
    "aggregate_metrics = {\n",
    "    \"KL Divergence (weighted)\": kl_weighted,\n",
    "    \"JS Divergence (weighted)\": js_weighted,\n",
    "    \"Total Variation Distance (mean)\": tv_average,\n",
    "    \"Wasserstein Distance (mean)\": w_average,\n",
    "    \"KS Test (mean)\": ks_average,\n",
    "    \"KS p-value (combined)\": ks_combined_p,\n",
    "    \"Total Samples\": total_n\n",
    "}\n",
    "\n",
    "# Print aggregated metrics\n",
    "print(\"\\n--- Aggregated Metrics ---\")\n",
    "for key, val in aggregate_metrics.items():\n",
    "    print(f\"{key}: {val:.4f}\")\n",
    "\n",
    "# ---- Display in a New Figure ----\n",
    "if plot:\n",
    "    fig_summary, ax_summary = plt.subplots(figsize=(5, 4))\n",
    "    ax_summary.axis(\"off\")\n",
    "\n",
    "    summary_text = '\\n'.join([\n",
    "        f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\"\n",
    "        for k, v in aggregate_metrics.items()\n",
    "    ])\n",
    "\n",
    "    ax_summary.text(0.5, 0.5, summary_text,\n",
    "                    fontsize=11,\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.9))\n",
    "\n",
    "    ax_summary.set_title(\"Aggregated Fit Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "1176debdbc0f67bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building the ctmc and running it",
   "id": "f6978a863c754ab1"
  },
  {
   "cell_type": "code",
   "id": "96be41a16a217a8",
   "metadata": {},
   "source": [
    "subset_el = pm4py.convert_to_event_log(df)\n",
    "subset_el = log_parser.add_start_end(subset_el)\n",
    "dfg, start_activities, end_activities = pm4py.discover_dfg(subset_el)\n",
    "dfg[\"end\", \"start\"] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1a301b90d3dd19e",
   "metadata": {},
   "source": [
    "if epsilon<10:\n",
    "    pm4py.view_dfg(dfg, start_activities, end_activities)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b13a70b666c2a8a",
   "metadata": {},
   "source": [
    "subset_el = pm4py.convert_to_dataframe(subset_el)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "418f1fe7074ccb93",
   "metadata": {},
   "source": [
    "data_transition_role_frequency = sim_util.get_transition_resource_dict(subset_el)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e84c6508fe0da157",
   "metadata": {},
   "source": [
    "for (e_from,e_to,role) in tuples_to_discard:\n",
    "    if e_from in data_transition_role_frequency:\n",
    "        if e_to in data_transition_role_frequency[e_from]:\n",
    "            if role in data_transition_role_frequency[e_from][e_to]:\n",
    "                data_transition_role_frequency[e_from][e_to].pop(role)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce44771e0e586256",
   "metadata": {},
   "source": [
    "for e_from in data_transition_role_frequency.keys():\n",
    "    for e_to in data_transition_role_frequency.keys():\n",
    "        if (e_from == 'start' and e_to == 'start') or (e_from == 'end' and e_to == 'end'):\n",
    "            data_transition_role_frequency[e_from].pop(e_to)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f660e252723ea3d",
   "metadata": {},
   "source": [
    "def remove_empty_keys(d):\n",
    "    \"\"\"Recursively remove empty keys from a three-level nested dictionary.\"\"\"\n",
    "    if not isinstance(d, dict):\n",
    "        return d  # Return non-dict values as they are\n",
    "\n",
    "    cleaned_dict = {}\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            cleaned_value = remove_empty_keys(value)  # Recursively clean sub-dictionaries\n",
    "            if cleaned_value:  # Add only if not empty\n",
    "                cleaned_dict[key] = cleaned_value\n",
    "        elif value not in (None, \"\", [], {}, ()):  # Ignore empty values\n",
    "            cleaned_dict[key] = value\n",
    "\n",
    "    return cleaned_dict\n",
    "\n",
    "data_transition_role_frequency = remove_empty_keys(data_transition_role_frequency)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9521bb4d2101842",
   "metadata": {},
   "source": [
    "role_resources = sim_util.get_detailed_weighted_role(subset_el)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f89c08e0ea52aee0",
   "metadata": {},
   "source": "role_trials = {k:v for k,v in role_resources.items()}",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "279afab90356e582",
   "metadata": {},
   "source": [
    "res = {}\n",
    "out_frequency = {}\n",
    "data_transition_role_prob = {}\n",
    "\n",
    "for k,v in data_transition_role_frequency.items():\n",
    "    if k in ['start','end']:\n",
    "        continue\n",
    "    out_freq = 0\n",
    "    if k not in data_transition_role_prob:\n",
    "        data_transition_role_prob[k] = {}\n",
    "\n",
    "    for k2,v2 in v.items():\n",
    "        if k2 in ['start','end']:\n",
    "            continue\n",
    "        all_freq = 0\n",
    "\n",
    "        if k2 not in data_transition_role_prob[k]:\n",
    "            data_transition_role_prob[k][k2] = {}\n",
    "\n",
    "        if k not in res:\n",
    "            res[k] = {}\n",
    "        if k2 not in res[k]:\n",
    "            for k3,v3 in v2.items():\n",
    "                if k3 not in data_transition_role_prob[k][k2]:\n",
    "                    data_transition_role_prob[k][k2][k3] = v3\n",
    "                all_freq += v3\n",
    "            res[k][k2] = all_freq\n",
    "            out_freq += all_freq\n",
    "        out_frequency[k] = out_freq\n",
    "\n",
    "for k,v in res.items():\n",
    "    for k2,v2 in v.items():\n",
    "        res[k][k2] = res[k][k2]/out_frequency[k]\n",
    "\n",
    "for k,v in data_transition_role_prob.items():\n",
    "    for k2,v2 in v.items():\n",
    "        for k3,v3 in v2.items():\n",
    "            data_transition_role_prob[k][k2][k3] = v3/out_frequency[k]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd6b38808cd08069",
   "metadata": {},
   "source": [
    "states = set(subset_el['concept:name'].unique()).difference(set(['start','end']))\n",
    "n = len(states)\n",
    "i = 0\n",
    "correspondence = {s:i for s,i in zip(states,range(len(states)))}\n",
    "#TODO: make sure none of the final states have state = 0 in the prism program\n",
    "non_final_states = list(states.difference(set(final_states)))\n",
    "for s in final_states:\n",
    "    if correspondence[s] == 0:\n",
    "        correspondence[s] = correspondence[non_final_states[0]]\n",
    "        correspondence[non_final_states[0]] = 0\n",
    "sorted(correspondence)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if epsilon<10:\n",
    "    view_resource_markov_chain(data_transition_role_prob)\n",
    "view_non_resource_markov_chain(res)"
   ],
   "id": "7921c60bf5d2b9ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the histogram (bar chart)\n",
    "fig, ax = plt.subplots(figsize=(8, 2))\n",
    "roles = list(role_resources.keys())\n",
    "values = list(role_resources.values())\n",
    "\n",
    "# Bar chart\n",
    "bars = ax.bar(roles, values, color='skyblue')\n",
    "\n",
    "# Define a top threshold (e.g., 95% of max value for the y-axis)\n",
    "y_max = max(values) * 1.05  # Increase y-limit for spacing\n",
    "ax.set_ylim(0, y_max)\n",
    "threshold = y_max * 0.9\n",
    "\n",
    "# Add point and value label smartly\n",
    "for bar, value in zip(bars, values):\n",
    "    x = bar.get_x() + bar.get_width() / 2\n",
    "    y = bar.get_height()\n",
    "    ax.plot(x, y, 'o', color='red')  # Red dot\n",
    "\n",
    "    # Decide label position\n",
    "    if y > threshold:\n",
    "        va = 'top'\n",
    "        y_offset = -0.08\n",
    "    else:\n",
    "        va = 'bottom'\n",
    "        y_offset = 0.08\n",
    "\n",
    "    ax.text(x, y + y_offset, f\"{value:.2f}\", ha='center', va=va, fontsize=14, color='black')\n",
    "\n",
    "# Labels and grid with increased font size\n",
    "ax.set_xlabel('Roles', fontsize=14)\n",
    "ax.set_ylabel('# Resources', fontsize=14)\n",
    "ax.tick_params(axis='x', labelsize=14)  # Increase x-axis (role labels) font size\n",
    "ax.tick_params(axis='y', labelsize=12)  # (Optional) Increase y-axis tick label size\n",
    "# ax.set_title('BPIC 13 Resource allocation', fontsize=16)\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Save the plot\n",
    "plot_filename = \"plots/bpic13roles.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_filename)\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "id": "29b4eb7c61b008c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "role_trials",
   "id": "7a83d06900864089",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37e2744b2d86df13",
   "metadata": {},
   "source": [
    "from simulation.ctmc_frequency import create_prism_program_from_log\n",
    "\n",
    "probabilities = create_prism_program_from_log(\n",
    "                            correspondence,\n",
    "                            final_states,\n",
    "                            data_mean_transition_role_time,\n",
    "                            role_resources,\n",
    "                            data_transition_role_frequency,\n",
    "                            role_trials,\n",
    "                            'ctmc-bpic13.sm',\n",
    "                            show_print=False)\n",
    "prism_program = stormpy.parse_prism_program('ctmc-bpic13.sm',prism_compat=True,simplify=True)\n",
    "model = stormpy.build_model(prism_program)\n",
    "\n",
    "def get_result(model, prism_program):\n",
    "    labels = \"\"\n",
    "    for fs in final_states:\n",
    "        labels += f'\"q_terminal_{fs}\" |'\n",
    "    labels = labels[:-2]\n",
    "    formula_str = f'Tmin=? [F {labels}]'\n",
    "    print(f\"[!] Formula: {formula_str}\")\n",
    "    properties = stormpy.parse_properties(formula_str, prism_program)\n",
    "    result = stormpy.model_checking(model, properties[0])\n",
    "    initial_state = model.initial_states[0]\n",
    "    result = result.at(initial_state)\n",
    "    print(f\"Duration: {timedelta(hours=result)}\")\n",
    "    return result\n",
    "\n",
    "result = get_result(model,prism_program)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis between ground truth, result and metrics",
   "id": "39c98301831e10dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Resource Regression Analysis",
   "id": "89e12f766c3db2c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "durations = []\n",
    "# x = list(range(1,50))\n",
    "samples = 1000\n",
    "for i in range(samples):\n",
    "    regression_role_trials = {}\n",
    "    for k,v in role_trials.items():\n",
    "        # random_resource_number = abs(random.gauss(v,v/2))\n",
    "        random_resource_number = abs(random.uniform(v/2,v*2))\n",
    "        regression_role_trials[k] = random_resource_number\n",
    "    probabilities = create_prism_program_from_log(\n",
    "                            correspondence,\n",
    "                            final_states,\n",
    "                            data_mean_transition_role_time,\n",
    "                            role_resources,\n",
    "                            data_transition_role_frequency,\n",
    "                            regression_role_trials,\n",
    "                            'ctmc-bpic13-temp.sm')\n",
    "    prism_program = stormpy.parse_prism_program('ctmc-bpic13-temp.sm', prism_compat=True, simplify=True)\n",
    "    model = stormpy.build_model(prism_program)\n",
    "    labels = \"\"\n",
    "    for fs in final_states:\n",
    "        labels += f'\"q_terminal_{fs}\" |'\n",
    "    labels = labels[:-2]\n",
    "\n",
    "    formula_str = f'Tmin=? [F {labels}]'\n",
    "    properties = stormpy.parse_properties(formula_str, prism_program)\n",
    "    result = stormpy.model_checking(model, properties[0])\n",
    "    initial_state = model.initial_states[0]\n",
    "    result = result.at(initial_state)\n",
    "    durations.append({**regression_role_trials, \"duration\": result})\n",
    "    # print(f'{i}/{samples}')"
   ],
   "id": "153b9fa0424a927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "durations_df = pd.DataFrame(durations)\n",
    "durations_df"
   ],
   "id": "85f6a30ca6be1715",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example: Load your DataFrame\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "def run_linear_regression(df, target_column, fit_intercept=True):\n",
    "    # Split into X (features) and y (target)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Fit model\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Print model coefficients\n",
    "    intercept = model.intercept_\n",
    "    coef = model.coef_\n",
    "    print(\"Intercept:\", intercept if fit_intercept else \"Not used\")\n",
    "    print(\"Coefficients:\")\n",
    "    for col, weight in zip(X.columns, coef):\n",
    "        print(f\"{col}: {weight:.4f}\")\n",
    "\n",
    "    return model, X.columns, coef\n",
    "\n",
    "def rank_features_by_importance(X, coef):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X,\n",
    "        'Coefficient': coef,\n",
    "        'Importance (abs)': abs(coef)\n",
    "    })\n",
    "    return importance_df.sort_values(by='Importance (abs)', ascending=False)\n",
    "\n",
    "# Run regression\n",
    "model, features, coefs = run_linear_regression(durations_df, 'duration', fit_intercept=True)\n",
    "\n",
    "# Rank features\n",
    "ranking = rank_features_by_importance(features, coefs)\n",
    "print(\"\\nFeature Ranking:\\n\", ranking)"
   ],
   "id": "2c4d60f2626ffc3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# pm4py values\n",
    "\n",
    "kalenkova values range between 12d1h to 8d22h in the optimized analysis"
   ],
   "id": "8b3c68bbe575d3c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(pm4py.get_cycle_time(subset_el))\n",
    "mean, median, margin_of_error = sim_util.get_pm4py_reference_times(subset_el)\n",
    "print('median',timedelta(seconds=median))\n",
    "print('mean',timedelta(seconds=mean))\n",
    "print('+- error:',timedelta(seconds=margin_of_error))"
   ],
   "id": "f28af840d6ed6b18",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
